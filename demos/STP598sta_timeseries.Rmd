---
title: "STP598sta: Spatiotemporal Analysis"
subtitle: "Time Series"
author: "Shiwei Lan"
date: Fall 2020
output:
  html_document:
    fontsize: 16pt
    toc: true
    toc_float: true
header-includes:
- \usepackage{amsmath}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
options(width = 1000)
```

In lectures 7 and 8, we have introduced time series and ARIMA models. We will discuss some examples in this demo.
We will particularly use the `R` package `astsa`.

```{r, eval=FALSE}
install.packages('astsa')
```

```{r}
library(astsa)
library(xts)
set.seed(2020)
```

## Introduction to Time Series

### Examples

First we plot some time series to show the challenges including non-stationarity, cyclic trend and multivariate time series.

```{r, fig.height = 5, fig.width = 9}
layout(matrix(c(1,1,2,3,4,5), nrow=2, ncol=3))
# non-stationary time series
tsplot(globtemp, type="o", ylab="Global Temperature Deviations")
# cyclic pattern
tsplot(soi, ylab="", main="Southern Oscillation Index (SOI)")
tsplot(rec, ylab="", main="Recruitment") 
# multivariate time series
ts.plot(fmri1[,2:5], col=1:4, ylab="BOLD", xlab="", main="Cortex")
ts.plot(fmri1[,6:9], col=1:4, ylab="BOLD", xlab="", main="Thalamus & Cerebellum")
mtext("Time (1 pt = 2 sec)", side=1, line=2) 
```

We also investigated the moving average model $v_t = \frac13 (w_{t-1}+w_t + w_{t+1})$.
In `R`, it can be plotted using `filter` function.

```{r, fig.height = 5, fig.width = 9}
w = rnorm(500,0,1)  # 500 N(0,1) variates
v = filter(w, sides=2, rep(1/3,3))  # moving average
par(mfrow=c(2,1))
tsplot(w, main="white noise")
tsplot(v, ylim=c(-3,3), main="moving average")
```

And the autoregressive model $x_t = x_{t-1} -0.9 x_{t-2} + w_t$:

```{r, fig.height = 3, fig.width = 9}
w = rnorm(550,0,1)  # 50 extra to avoid startup problems
x = filter(w, filter=c(1,-.9), method="recursive")[-(1:50)]
tsplot(x, main="autoregression")
```

And the random walk with drift model $x_t = \delta +x_{t-1} +w_t$:

```{r, fig.height = 3, fig.width = 9}
w = rnorm(200); x = cumsum(w) # two commands in one line
wd = w +.2;    xd = cumsum(wd)
tsplot(xd, ylim=c(-5,55), main="random walk", ylab='')
lines(x, col=4) 
abline(h=0, col=4, lty=2)
abline(a=0, b=.2, lty=2)
```

### Measures of dependence

Cross-correlation function (CCF) $\rho_{xy}(h) = \frac{\gamma_{xy}(h)}{\sqrt{\gamma_x(0)\gamma_y(0))}}$ can be used to measure the dependence of time series. For example, $y_t = A x_{t-\ell} + w_t$ is a lagged model where $y_t$ lags behind $x_t$ for $\ell$ time units. In `R`, we could use `ccf` function to plot CCF.

```{r, fig.height = 3, fig.width = 9}
x = rnorm(100)
y = lag(x, -5) + rnorm(100)
ccf(y, x, ylab='CCovF', type='covariance')
abline(v=0, lty=2)
text(11, .9, 'x leads')
text(-9, .9, 'y leads')
```

### Estimation of Correlation

To measure the correlation of a time series to itself, we could consider the auto-correlation function (ACF) $\rho(h) = \frac{\gamma(h)}{\gamma(0)}$. In `R`, we could use `acf` function to compute its sample version. Let's use the speech data for example.

```{r, fig.height = 5, fig.width = 9}
par(mfrow=c(2,1))
# Speech recording of the syllable aaa ... hhh 
tsplot(speech)
# ACF of the speech seires (upto lag 250)
acf1(speech, 250)
```

We could also use `ccf` function to estimate the CCF of two time series, e.g. SOI vs Recruitment.

```{r, fig.height = 6, fig.width = 9}
par(mfrow=c(3,1))
acf(soi, 48, main="Southern Oscillation Index")
acf(rec, 48, main="Recruitment")
ccf2(soi, rec, 48, main="SOI vs Recruitment")
```

### Classical Regression in Time Series

To review the classical regression and basic exploratory data analysis in time series analysis, we consider the monthly price (per pound) of a chicken in the US from mid-2001 to mid-2016 (180 months).
There is an obvious upward trend in the series, and we might use simple linear regression to estimate that trend by fitting the model
$$x_t = \beta_0 + \beta_1 z_t +w_t, \quad z_t=2001\frac{7}{12}, 2001\frac{8}{12}, \cdots 2006\frac{6}{12}$$.

```{r, fig.height = 3, fig.width = 9}
summary(fit <- lm(chicken~time(chicken))) # regress price on time
tsplot(chicken, ylab="cents per pound", col=4, lwd=2)
abline(fit)           # add the fitted regression line to the plot
```

After we remove the fitted linear trend $\hat\mu_t = -7131 + 3.59t$, we obtain the residual $\hat y_t = x_t- \hat\mu_t$.
Alternatively, we could use differencing to remove the trend and plot $\nabla x_t$.

```{r, fig.height = 5, fig.width = 9}
par(mfrow=c(2,1))
tsplot(resid(fit), main="detrended")
tsplot(diff(chicken), main="first difference")
```

Furthermore, we could investigate the autocorrelation of the resulted time series.

```{r, fig.height = 6, fig.width = 9}
par(mfrow=c(3,1))     # plot ACFs
acf(chicken, 48, main="chicken")
acf(resid(fit), 48, main="detrended")
acf(diff(chicken), 48, main="first difference")
```

When there are multiple independent time series, we can fit classical regression models of the dependent time series and do model selection.
Consider a study of the possible effects of temperature and pollution on weekly mortality in Los Angeles County. Note the strong seasonal components in all of the series, corresponding to winter-summer variations and the downward trend in the cardiovascular mortality over the 10-year period.

```{r, fig.height = 6, fig.width = 9}
par(mfrow=c(3,1))
tsplot(cmort, main="Cardiovascular Mortality", ylab="")
tsplot(tempr, main="Temperature",  ylab="")
tsplot(part, main="Particulates", ylab="")

#  Regression
temp  = tempr-mean(tempr)  # center temperature    
temp2 = temp^2             # square it  
trend = time(cmort)        # time

fit = lm(cmort~ trend + temp + temp2 + part, na.action=NULL)
            
summary(fit)       # regression results
summary(aov(fit))  # ANOVA table   (compare to next line)
summary(aov(lm(cmort~cbind(trend, temp, temp2, part)))) # Table 2.1

num = length(cmort)                                     # sample size
AIC(fit)/num - log(2*pi)                                # AIC 
BIC(fit)/num - log(2*pi)                                # BIC 
# AIC(fit, k=log(num))/num - log(2*pi)                  # BIC (alt method)    
(AICc = log(sum(resid(fit)^2)/num) + (num+5)/(num-5-2)) # AICc
```

### Lagged Regression

Recall that the Southern Oscillation Index (SOI) measured at time $t âˆ’ 6$ months is associated with the Recruitment series at time $t$, indicating that the SOI leads the Recruitment series by six months. We consider the following lagged regression
$$R_t = \beta_0 + \beta_1 S_{t-6} + w_t$$

Performing lagged regression in `R` is a little difficult because the series must be aligned prior to running the regression. The easiest way to do this is to create a data frame (that we call `fish`) using `ts.intersect`, which aligns the lagged series.

```{r, fig.height = 3, fig.width = 9}
fish = ts.intersect(rec, soiL6=lag(soi,-6), dframe=TRUE)   
summary(fit <- lm(rec~soiL6, data=fish, na.action=NULL))
tsplot(fish$rec, ylim=c(0,111))  # plot the data and the fitted values (not shown in text) 
lines(fitted(fit), col=2)
```

In addition to transformation, we consider another preliminary data processing technique that is used for the purpose of visualizing the relations between series at different lags, namely, *scatterplot matrices*.

Note ACF investigates the linear relationship between $x_t$ and $x_{t-h}$. It gives a profile of the linear correlation at all possible lags and shows which values of $h$ lead to the best predictability. However, there may be nonlinear relationship between $x_t$ and $x_{t-h}$ being masked. this leads to the idea of examining scatterplots, possibly $y_t$ versus $x_{t-h}$.

Let's again consider the scatterplots of Southern Oscillation Index (SOI) and Recruitment series.

```{r, fig.height = 6, fig.width = 8}
lag1.plot(soi, 12)
```

```{r, fig.height = 6, fig.width = 8}
lag2.plot(soi, rec, 8)
```

We noticed that the relationship is nonlinear and different when SOI is positive or negative. To account for that, we fit the model
$$R_t = \beta_0 +\beta_1 S_{t-6} + \beta_2 D_{t-6} + \beta_3 D_{t-6} S_{t-6} +w_t$$
where $D_t$ is a dummy variable that is $0$ if $S_t<0$ and $1$ otherwise, i.e. $D_t = I(S_t\geq 0)$.

```{r, fig.height = 8, fig.width = 9}
dummy = ifelse(soi<0, 0, 1)
fish  = ts.intersect(rec, soiL6=lag(soi,-6), dL6=lag(dummy,-6), dframe=TRUE)
summary(fit <- lm(rec~ soiL6*dL6, data=fish, na.action=NULL))
layout(matrix(c(1,1,2,3),nrow=2,ncol=2,byrow=T))
attach(fish)
plot(soiL6, rec)
lines(lowess(soiL6, rec), col=4, lwd=2)
points(soiL6, fitted(fit), pch='+', col=2)
tsplot(resid(fit)) # not shown ...
acf(resid(fit))   # ... but obviously not noise
```


### Smoothing

Smoothing is useful in discovering certain traits in a time series, such as long-term trend and seasonal components.
In particular, we could use moving average as a filter to smooth monthly SOI series with
$$m_t = \sum_{j=-k}^k a_j x_{t-j}$$
with $a_0 = a_{\pm 1}=\cdots= a_{\pm 5}=1/2$ and $a_{\pm 6} = 1/24$; $k=6$.

```{r, fig.height = 4, fig.width = 9}
wgts = c(.5, rep(1,11), .5)/12
soif = filter(soi, sides=2, filter=wgts)
tsplot(soi)
lines(soif, lwd=2, col=4)
par(fig = c(.75, 1, .75, 1), new = TRUE) # the insert
nwgts = c(rep(0,20), wgts, rep(0,20))
plot(nwgts, type="l", ylim = c(-.02,.1), xaxt='n', yaxt='n', ann=FALSE)
```

Alternatively, one can consider the kernel smoothing using `ksmooth` function or the locally weighted scatterplot smoothing (lowess) using `lowess` function.

```{r, fig.height = 4, fig.width = 9}
# Gaussian kernel smoother
tsplot(soi)
lines(ksmooth(time(soi), soi, "normal", bandwidth=1), lwd=2, col=4)
par(fig = c(.75, 1, .75, 1), new = TRUE) # the insert
gauss = function(x) { 1/sqrt(2*pi) * exp(-(x^2)/2) }
x = seq(from = -3, to = 3, by = 0.001)
plot(x, gauss(x), type ="l", ylim=c(-.02,.45), xaxt='n', yaxt='n', ann=FALSE)
```

```{r, fig.height = 4, fig.width = 9}
# lowess
tsplot(soi)
lines(lowess(soi, f=.05), lwd=2, col=4) # El Nino cycle
lines(lowess(soi), lty=2, lwd=2, col=2) # trend (with default span)
```

## ARIMA Models

In this section, we review AR(p), MA(q), ARMA(p, q), ARIMA(p, d, q) models.

### Autoregressive Models AR(p)

First, we look at two simulated AR(1) models with $\phi=0.9$ and $\phi=-0.9$ respectively.

```{r, fig.height = 5, fig.width = 9}
par(mfrow=c(2,1))                         
# in the expressions below, ~ is a space and == is equal
tsplot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab="x", main=(expression(AR(1)~~~phi==+.9))) 
tsplot(arima.sim(list(order=c(1,0,0), ar=-.9), n=100), ylab="x", main=(expression(AR(1)~~~phi==-.9))) 
```


### Moving Average Models MA(q)

Next, we introduced two simulated MA(1) models with $\theta=0.9$ and $\theta=-0.9$ resepectively.

```{r, fig.height = 5, fig.width = 9}
par(mfrow=c(2,1))                                   
tsplot(arima.sim(list(order=c(0,0,1), ma=.9), n=100), ylab="x", main=(expression(MA(1)~~~theta==+.9)))    
tsplot(arima.sim(list(order=c(0,0,1), ma=-.9), n=100), ylab="x", main=(expression(MA(1)~~~theta==-.9)))    
```

### Autoregressive Moving Average Models ARMA(p,q)

We can fit a time series with specified degrees $p$, $q$ using `arima` function. 
The model below has redundant parameters $$(1+ .96B) x_t = (1+ .95B) w_t $$

```{r}
set.seed(8675309)         # Jenny, I got your number
x = rnorm(150, mean=5)    # generate iid N(5,1)s
arima(x, order=c(1,0,1))  # enstimation
```

We presented several methods (back substitution, Taylor expansion, mathcing coefficients) to obtain MA representation of ARMA models ($\psi$ weights in $x_t = \psi(B) w_t$ and $\pi$ weights in $\pi(B) x_t = w_t$ respectively). But in `R`, we could simply use `ARMAtoMA` to obtain them.

Consider ARMA(1,1) model $(1-.9B) x_t = (1+.5B)w_t$ which is both causal and invertible. Using the method of matching coefficients and solving the resulted difference equations, we obtained $$\psi = 1.4* 0.9^{j-1}, \quad \pi_j = 1.4 * (-0.5)^{j-1}$$

```{r}
ARMAtoMA(ar = .9,  ma = .5,  10)   # first 10 psi-weights
ARMAtoMA(ar = -.5, ma = -.9, 10)   # first 10 pi-weights
```

### Autocorrelation and Partial Autocorrelation

In the lecture 8, we mentioned that ACF alone alone tells us little about the orders of dependence for autoregressive type models thus partial autocorrelation function PACF is introduced.

Consider AR(2) model $x_t = 1.5 x_{t-1} - 0.75 x_{t-2} +w_t$. We can use `ARMAacf` function to compute ACF and PACF for ARMA seriers.

```{r, fig.height = 6, fig.width = 9}
z = c(1,-1.5,.75)    # coefficients of the polynomial
(a = polyroot(z)[1]) # = 1+0.57735i,  print one root which is 1 + i 1/sqrt(3)
arg = Arg(a)/(2*pi)  # arg in cycles/pt  
1/arg                # = 12,  the period

layout(matrix(c(1,1,2,3),nrow=2,ncol=2,byrow=T))

set.seed(8675309)    # Jenny, it's me again
ar2 = arima.sim(list(order=c(2,0,0), ar=c(1.5,-.75)), n = 144)
plot(ar2, axes=FALSE, xlab="Time")
axis(2); axis(1, at=seq(0,144,by=12)); box()  # work the plot machine
abline(v=seq(0,144,by=12), lty=2)
# ACF
ACF = ARMAacf(ar=c(1.5,-.75), ma=0, 24)
plot(ACF, type="h", xlab="lag")
abline(h=0)
# PACF
PACF = ARMAacf(ar=c(1.5,-.75), ma=0, 24, pacf=TRUE)
plot(PACF, type="h", xlab="lag")
abline(h=0)
```

Given a time series, e.g. the Recruitment series, we could use `acf2` from `astsa` to print and plot empirical estimates of ACF and PACF simultaneously (Or use built-in function `acf` and `pacf` respectively).

```{r, fig.height = 6, fig.width = 9}
acf2(rec, 48)     # will produce values and a graphic
```

### Forecasting

In lecture 8, we briefly mentioned the mean squared one-step-ahead forecast and its prediction error.
In practice, we could fit OLS by `ar.ols` and use `predict` function to forecast. We could also obtain $(1-\alpha)$ prediction interval of the form $$x_{n+m}^n \pm c_{\frac{\alpha}{2} \sqrt{P_{n+m}^n}$$
In the following forecasting problem, we use the Recruitment series. From the previous plot, we see that $p=2$.

```{r, fig.height = 4, fig.width = 9}
regr = ar.ols(rec, order=2, demean=F, intercept=TRUE)  # regression
# regr$asy.se.coef  # standard errors
fore = predict(regr, n.ahead=24)
ts.plot(rec, fore$pred, col=1:2, xlim=c(1980,1990), ylab="Recruitment")
U = fore$pred+fore$se; L = fore$pred-fore$se
xx = c(time(U), rev(time(U))); yy = c(L, rev(U)) 
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2)) 
lines(fore$pred, type="p", col=2)
```

### Estimation

In fitting an ARMA(p,q) model to a given time series, we could first use ACF and PACF to determine $q$ and $p$ respectively. To estimate the regressive polynomial and moving average polynomial, we could use Yule-Walker estimator (for AR(p), in lecture 8), MLE etc..

Let's consider the Recruitment series. We use Yule-Walker estimator to fit AR(2) model and compare the prediction with previous OLS model.

```{r, fig.height = 4, fig.width = 9}
rec.yw = ar.yw(rec, order=2)
rec.yw$x.mean  # = 62.26278 (mean estimate)
rec.yw$ar      # = 1.3315874, -.4445447  (parameter estimates)
sqrt(diag(rec.yw$asy.var.coef))  # = .04222637, .04222637  (standard errors)
rec.yw$var.pred  # = 94.79912 (error variance estimate)

rec.pr = predict(rec.yw, n.ahead=24)
U = rec.pr$pred + rec.pr$se
L = rec.pr$pred - rec.pr$se
minx = min(rec,L); maxx = max(rec,U)
ts.plot(rec, rec.pr$pred, xlim=c(1980,1990), ylim=c(minx,maxx)) 
xx = c(time(U), rev(time(U))); yy = c(L, rev(U)) 
polygon(xx, yy, border = 8, col = gray(.6, alpha = .2)) 
lines(rec.pr$pred, type="p", col=2)
```


### Autoregressive Integrated Moving Average Models ARIMA(p,d,q)

Lastly, let's look at ARIMA model.
The ARIMA(0,1,1), or IMA(1,1) model is of interest because many economic time series can be successfully modeled this way. In addition, the model leads to a frequently used, and abused, forecasting method called exponentially weighted moving averages (EWMA). We will write the model as
$$x_t = x_{t-1} + w_t - \lambda w_{t-1}$$
with $|\lambda|<1$ and $x_0=0$.
By writing $y_t = \nabla x_t$ we can show that $$x_t = \sum_{j=1}^\infty (1-\lambda)\lambda^{j-1} x_{t-j} + w_t$$
The truncated forecasts are $$\tilde x_{n+1}^n = (1-\lambda)x_n + \lambda \tilde x_n^{n-1}, \quad P_{n+m}^n \approx \sigma^2_w [1+(m-1)(1-\lambda)^2]$$
In EWMA, the parameter $1 -\lambda$  is often called the smoothing parameter and larger values of $\lambda$ lead to smoother forecasts.
In `R`, this is accomplished using the `HoltWinters` command.

```{r, fig.height = 4, fig.width = 9}
set.seed(666)    
x = arima.sim(list(order = c(0,1,1), ma = -0.8), n = 100)
(x.ima = HoltWinters(x, beta=FALSE, gamma=FALSE))  # Î± is 1-Î» here
plot(x.ima)
```

Finally, we close with fitting an ARIMA model to US GNPData.
In this example, we consider the analysis of quarterly U.S. GNP from 1947(1) to 2002(3), n = 223 observations. The data are real U.S. gross national product in billions of chained 1996 dollars and have been seasonally adjusted. The data were obtained from the Federal Reserve Bank of St. Louis [http://research.stlouisfed.org/](http://research.stlouisfed.org/).

```{r, fig.height = 4, fig.width = 9}
# par(mfrow=c(2,1))
plot(gnp)
acf2(gnp, 50)
```

Consider log transformation and differencing.

```{r, fig.height = 4, fig.width = 9}
# par(mfrow=c(2,1))
gnpgr = diff(log(gnp))      # growth rate
plot(gnpgr)
acf2(gnpgr, 24)
```

Now we do diagnostics.

```{r, fig.height = 6, fig.width = 9}
# par(mfrow=c(2,1))
sarima(gnpgr, 1, 0, 0) # AR(1)
sarima(gnpgr, 0, 0, 2) # MA(2)
ARMAtoMA(ar=.35, ma=0, 10) # prints psi-weights
```