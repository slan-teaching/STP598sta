---
title: "Bayesian Modeling with MCMC"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Example

Let's consider a Bayesian linear regression example.

$$
y | \beta, \sigma^2 \sim N(X\beta, \sigma^2 I_N)\\
\beta \sim N(0, \sigma^2_0 I_p)
$$
Suppose we use the above `cars` as an example and model `dist` as a linear function of `speed`.

```{r, echo=F}
sigma2 = var(cars$dist)
sigma2_0 = 100
```
We fix $\sigma^2$ at `r round(sigma2,2)` and choose $\sigma^2_0$ to be relatively large, e.g. $\sigma^2_0=`r sigma2_0`$.

Now we define the log-posterior of $\beta$ as
```{r}
X = cbind(1,cars$speed)
y = cars$dist
logpost = function(beta){
  loglik = -0.5*sum((y-X%*%beta)^2/sigma2)
  logpri = -0.5*sum(beta^2/sigma2_0)
  logpost = loglik + logpri
  return(logpost)
}
```

Now we run random walk Metropolis (RMW) to generate samples.

```{r}
set.seed(2020)
source('RWM.R')
source('slice.R')

# set step size
eps = c(2.,.2)

# allocate space to store samples
n_samp = 10000
n_burn = 5000
samp = matrix(NA,n_samp,2)
accp = 0 # online acceptance
acpt = 0 # accumulative acceptance

# set initial point
beta = rnorm(2, sd=sqrt(sigma2_0))

U = function(beta)-logpost(beta)
u = U(beta)
logf_k = function(beta_k,k){
  beta[k] = beta_k
  return(logpost(beta))
}

# generate samples
for(i in 1:(n_burn+n_samp)){
  # RWM
  # if(i %% 100==0){
  #   print(paste('Online acceptannce rate at iteration ',i,' is: ',round(accp/100,2),sep=''))
  #   accp=0
  #   print(paste('Accumulative acceptannce rate at iteration ',i,' is: ',round(acpt/i,2),sep=''))
  # }
  # res = RWM(beta,u,U,eps)
  # beta = res$q; u = res$u
  # accp = accp+ res$Ind
  # acpt = acpt+ res$Ind
  
  # slice
  for(k in 1:length(beta)){
    beta[k] = slice(function(beta_k)logf_k(beta_k,k),beta[k])
  }
  
  # save samples
  if(i==n_burn) print('Burning completed. Saving samples...')
  if (i>n_burn){
    samp[i-n_burn,]=beta
  }
}
```

Now we have samples and we can check the convergence by `geweke` from `coda`.

```{r}
library(coda)
samp_mcmc=mcmc(samp)
diag = geweke.diag(samp_mcmc)
# p-value
pnorm(abs(diag$z),lower.tail=F)*2
```

And we can plot posterior samples.
```{r, fig.height = 6, fig.width = 9}
plot(samp_mcmc)
```

Finally we make posterior estimate and compare them with ordinary least square solution.
```{r}
summary(samp)
# summary(samp_mcmc)
ols = lm(dist~speed,cars)
summary(ols)
```